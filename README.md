# ODSML-Assignment

## Overview
The assignment covers foundational concepts and practical applications of optimization techniques, including:
- Gradient Descent
- Newton’s Method
- Hessian Analysis and Local Optimality
- Applied optimization on real functions and small datasets
- Comparison between solvers and conceptual understanding of Quasi-Newton methods

## Part A Warm-up
### A1. Gradient Descent Recap
- explain why the gradient descent update rule
  wk+1 = w<sup>k</sup> − α ∇f(wk)
leads us closer to a (local) minimum, provided α is suitably chosen.
- Give an example of how you might pick a fixed step size versus using a simple Armijo line
search. Briefly discuss one advantage of line search in practice.

### A2. Newton’s Method
f(x) = x⁴ - 4x² + 5
